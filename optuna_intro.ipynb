{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Z9NHT6_Eva"
      },
      "source": [
        "<img src=\"https://github.com/pfnet-research/optuna-hands-on/blob/master/en/files/logo.jpg?raw=1\"/>\n",
        "\n",
        "Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.\n",
        "\n",
        "- GitHub: https://github.com/pfnet/optuna\n",
        "- Document: https://optuna.readthedocs.io/en/stable/\n",
        "\n",
        "This notebook describes the basic usage of Optuna with simple optimization tasks of quadratic function and linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEomu83y6MCG"
      },
      "source": [
        "## Installation\n",
        "First of all, install Optuna by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow12BEEu6MRQ"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBSdeWPeniM"
      },
      "source": [
        "## Basic Usage\n",
        "Below is the basic usage of Optuna. You can immediately start an optimization task just filling the following template with your machine learning logic and the number of trials.\n",
        "\n",
        "```python\n",
        "import optuna\n",
        "\n",
        "def objective(trial):  # `trial` is an object passed by Optuna.\n",
        "    some_machine_learning_logic(trial)  # Write your machine learning logic here.\n",
        "    return evaluation_score  # Return the evaluation score of the trained model.\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=N_TRIALS)  # Specify the number of trials. \n",
        "```\n",
        "\n",
        "## Optimize Quadratic Function\n",
        "Before optimizing a machine learning model, let's see how Optuna solves a very simple task that minimizes the output of $f(x) = (x - 2)^2$.\n",
        "Although the answer is obviously $f(x) = 0$ when $x = 2$, Optuna doesn't know how to solve that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4u2HV_ceniN"
      },
      "outputs": [],
      "source": [
        "import optuna  # Remember to install optuna with `!pip install optuna` first.\n",
        "\n",
        "def objective(trial):\n",
        "    x = trial.suggest_uniform('x', -100, 100)\n",
        "    return (x - 2) ** 2\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obQKLYBZeniO"
      },
      "source": [
        "Executing the cell above, you should see 100 lines of execution log. Optuna calls `objective` 100 times changing the value of `x`, where the range of `x` is specified as $[-100, 100)$ in `trial.suggest_uniform('x', -100, 100)`. A `trial` is an object passed by Optuna, corresponds to a single call of `objective`, and provides interfaces to get next hyperparameter to be tried.\n",
        "\n",
        "Note that `objective` is a blackbox function for Optuna. The library only observes the input, `x`, and the output of the function.  The library gradually improves `x` with a smart internal algorithm (Bayesian optimization).\n",
        "\n",
        "You can access to the best result with `study.best_value` and `study.best_params`, which should be near $x = 2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmqSKTQzeniP"
      },
      "outputs": [],
      "source": [
        "print('Minimum objective value: ' + str(study.best_value))\n",
        "print('Best parameter: ' + str(study.best_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITbA9FEqeniQ"
      },
      "source": [
        "In summary, you need the following steps to set up the optimization.\n",
        "\n",
        "- Define the objective function that calculates the minimization/maximization target.\n",
        "- Inside the objective function, set the hyperparameters to be optimized with `suggest` methods.\n",
        "- Instantiate the `study` object.\n",
        "- Start the optimization with `study.optimize`, specifying the number of trials with `n_trials`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zuF5-nnvqBu"
      },
      "source": [
        "## Optimize Machine Learning Models\n",
        "\n",
        "Let's optimize the following machine learning logic, where a linear regression model (Lasso) is trained for the Boston Housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spk8G3Y9rxRv"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "\n",
        "# hyperparameter setting\n",
        "alpha = 1.0\n",
        "\n",
        "# data loading and train-test split\n",
        "X, y = sklearn.datasets.load_boston(return_X_y=True)\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, random_state=0)\n",
        "\n",
        "# model training and evaluation\n",
        "model = sklearn.linear_model.Lasso(alpha=alpha)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n",
        "\n",
        "# output: evaluation score\n",
        "print('Mean squared error: ' + str(error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxSmt_dueniT"
      },
      "source": [
        "Performance of Lasso regression is sensitive to the L1 constant, `alpha`, and it's tiresome for humans to manually search for the appropriate value. With Optuna, you can search for `alpha` as follows. Note that you just need to wrap the machine learning logic in the previous cell with `objective` and to set up `study` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO0XcBNOzuTR"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "\n",
        "def objective(trial):\n",
        "    # hyperparameter setting\n",
        "    alpha = trial.suggest_uniform('alpha', 0.0, 2.0)\n",
        "    \n",
        "    # data loading and train-test split\n",
        "    X, y = sklearn.datasets.load_boston(return_X_y=True)\n",
        "    X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, random_state=0)\n",
        "    \n",
        "    # model training and evaluation\n",
        "    model = sklearn.linear_model.Lasso(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n",
        "\n",
        "    # output: evaluation score\n",
        "    return error\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_C5V0vSHWTV"
      },
      "source": [
        "Let's see the best result among 20 trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42EbIIR5HVD7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print('Minimum mean squared error: ' + str(study.best_value))\n",
        "print('Best parameter: ' + str(study.best_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbRBz3gXeniV"
      },
      "source": [
        "To access the results of all trials, you can use `study.trials_dataframe`, which shows the details of trials as a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzDJWSKoeniV"
      },
      "outputs": [],
      "source": [
        "study.trials_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uruvcV6IraQ"
      },
      "source": [
        "## Imperative Interface: Search Conditional Hyperparameters\n",
        "\n",
        "Optuna deals with conditional hyperparameters with its imperative (define-by-run) interace.\n",
        "Suppose that you are wondering which regularization method is better: `Ridge` or `Lasso`. You also want to optimize the regularization constant of each method.\n",
        "In this case, you have three hyperparameters to be optimized.\n",
        "\n",
        "- `regression_method`: `'ridge'` or `'lasso'`\n",
        "- `ridge_alpha`: the regularization constant of `ridge`\n",
        "- `lasso_alpha`: the regularization constant of `lasso`\n",
        "\n",
        "Note that `ridge_alpha` and `lasso_alpha` are conditional hyperparameters:\n",
        "`ridge_alpha` appears in the search space only when `regression_method` is `ridge`; and `lasso_alpha` does only when `regression_method` is `lasso`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR4cCFXgeniW"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "\n",
        "def objective(trial):\n",
        "    # hyperparameter setting\n",
        "    regression_method = trial.suggest_categorical('regression_method', ('ridge', 'lasso'))\n",
        "    if regression_method == 'ridge':\n",
        "        ridge_alpha = trial.suggest_uniform('ridge_alpha', 0.0, 2.0)\n",
        "        model = sklearn.linear_model.Ridge(alpha=ridge_alpha)\n",
        "    else:\n",
        "        lasso_alpha = trial.suggest_uniform('lasso_alpha', 0.0, 2.0)\n",
        "        model = sklearn.linear_model.Lasso(alpha=lasso_alpha)\n",
        "    \n",
        "    # data loading and train-test split\n",
        "    X, y = sklearn.datasets.load_boston(return_X_y=True)\n",
        "    X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, random_state=0)\n",
        "\n",
        "    # model training and evaluation\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n",
        "  \n",
        "    # output: evaluation score\n",
        "    return error\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFifBtc8eniX"
      },
      "source": [
        "Let's see the optimization results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf4veC8weniX"
      },
      "outputs": [],
      "source": [
        "print('Minimum mean squared error: ' + str(study.best_value))\n",
        "print('Best parameter: ' + str(study.best_params))\n",
        "\n",
        "study.trials_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D02jdYD9enib"
      },
      "source": [
        "## Conclusion\n",
        "This notebook summarized the basic usage of Optuna and its imperative interface. To optimize an ML model, you just need to define an objective function that includes a usual logic of training and evaluation. See also the official [document](https://optuna.readthedocs.io/en/stable/index.html) and [tutorial](https://optuna.readthedocs.io/en/stable/tutorial/index.html) for more details."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Optuna Lecture",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
